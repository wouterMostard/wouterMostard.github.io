---
title: "Semantic Preserving Siamese Autoencoder for Binary Quantization of Word Embeddings"
collection: publications
permalink: /publication/nlpir_21
excerpt: 'Word embeddings are used as building blocks for a wide range of natural language processing and information retrieval tasks. These embeddings are usually represented as continuous vectors, requiring significant memory capacity and computationally expensive similarity measures. In this study, we introduce a novel method for semantic hashing continuous vector representations into lower-dimensional Hamming space while explicitly preserving semantic information between words. This is achieved by introducing a Siamese autoencoder combined with a novel semantic preserving loss function. We show that our quantization model induces only a4% loss of semantic information over continuous representations and outperforms the baseline models on several word similarity and sentence classification tasks. Finally, we show through cluster analysis that our method learns binary representations where individual bits hold interpretable semantic information. In conclusion, binary quantization of word embeddings significantly decreases time and space requirements while offering new possibilities through exploiting semantic information of individual bits in downstream information retrieval tasks.'
date: 2019-10-12
venue: '(Accepted at) NLPIR 2021'
paperurl: 'http://woutermostard.github.io/files/nlpir_paper.pdf'
citation: 'Mostard, Wouter. (2019). &quot;Semantic Preserving Siamese Autoencoder for Binary Quantization of Word Embeddings &quot; <i>5th International Conference on Natural Language Processing and Information Retrieval</i>'
---

[Download paper here](http://woutermostard.github.io/files/nlpir_paper.pdf)