---
title: 'WIP: Value based Reinforcement Learning in Acme'
date: 2021-07-12
---

Introduction
======

Reinforcement learning is one of the pilars of machine learning, next to unsupervised and supervised learning. In 
reinforcement learning you have an agent and an envvironment. The goal of the agent is to learn to pick the best action
given a state the the agent is in in the environment. Globally this environment can be seen as in the following diagram

<figure>
<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/500px-Reinforcement_learning_diagram.svg.png'>
<figcaption>Image taken from Wikipedia</figcaption>
</figure>

The agent performs an action given a state that has been observed and retrieves a reward signal. Based on this reward
signal the agent tries to learn the optimal mapping. Note that this type of learning is somewhere between unsupervised 
and supervised learning. The agent can retrieve "labels" in the form of a reward but these are only implicitely observed
from the environment. This type of learning is much closer to how we as humans for example learn. Thanks goes out to the 
organizers of EEML and Google Deepmind for hosting the EEML 2021 summer school.

the goal of this blog is to implement a simple agent learning to navigate to a goal state using the Acne library. Especially
we will focus on \textbf{value based methods} where the agent has some kind of internel state the remenbers all the state 
action pairs it has seen. 

An action pair is given by a action-value function 

$$
Q^\pi (s, a ) = E_{\tau \approx P^\pi} \sum_t \gamme^t R_t | s_0 = s, a_0 = a
$$

where $\tau$ is the set of all states, actions, and rewards retrieved, i.e. $\tau = {s_0, a_0, r_0, ..., s_n, a_n, r_n}$,
$\pi$ is a given policy that the agent is following. What the equation states is that given a initial state and action and all 
the other visited states and actions the ending value for the table is the sum of the rewards multiplied by $\gamma$. Here 
$\gamma$ is a discount factor for rewards obtained further away from the current state. 

It can be proven that these value estimators can be calculated using the famoud Bellman equation

$$
Q^\pi (s, a ) = r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^\pi(s')
$$

Where $V^\pi(s')$ is the expected value of a given state $s'$. Note that his definition is recursive since the value of 
state s1 and a0 are dependent on the value of s2 and all states given all other actions. 

*Temporal difference* is the difference between the actual action value for a given state and action and the true value that 
you would receive using a specific action in that state.  


Policy iteration is a method where you first evaluate a given policy on your problem. Subsequently you calculate the temporal
difference of that given policy and you change it slightly by this loss. You then *greedy* set the optimal action given a state
to be the action that gives the highest value from that state.

Problem with these approaches is that you are required to fit all the state action pairs into  memory. To account for this 
we would like to just calculate the function values, not all the actual scores. This can be done using *function approximation*
In this blog we look at Neural Fitted Q iteration. So what you basically do is that you represent $Q(s,a )$ using a neural network
which has as the input a state s and outputs Q-values for the possible actions 

Next up is a method where you train a deep neural network to go from a complex state s to actions. In tha atari example
you can for example use a convolutional neural network. 


Playing Atari games
======
One popular example where function approximation can be of use is in playing games. When playing games maintaining a state
action value for every possibility becomes intractable and we need to evaluate the valutation using a function. In this 
case we will look into playing a classical Atari game called Breakout. 

<figure>
    <img src="https://www.researchgate.net/profile/Luxin-Zhang-2/publication/331858099/figure/fig3/AS:803900928057345@1568676325982/State-action-mismatch-game-Breakout-game-speed-at-60Hz-The-state-s-0-at-time-t-0-is.ppm">
    <figcaption>https://www.researchgate.net/figure/State-action-mismatch-game-Breakout-game-speed-at-60Hz-The-state-s-0-at-time-t-0-is_fig3_331858099</figcaption>
</figure>

Here the goal is to use the bar at the bottom and a ball to remove all the blocks at the top. As you can imagine it is 
impossible to store the right action given each state (which will be the position of the ball and of the bar). This is based
on a paper from [1] where $Q(s, a)$ is approximated using a neural network $f(s)$ which gives as output the actions $a$
to perform. As described above we already have a good loss function, namely the temporal difference. The goal of the 
network will be to minimize the TD loss which can be given as the L2 loss

$$
    Loss = E[(r + y max_{a'}Q(s', a') - Q(s, a))^2]
$$ 

Updating this the model for every data point can be quite unstable. For this reason the paper uses a *replay buffer*. 
What a replay buffer basically does is that you pick a number of random actions and you accumulate all the observed transitions, 
i.e. all the states that the model went to and the reward given. This makes sure that you update the Q function given a batch
of state and action pairs. 

