---
title: 'WIP: Value based Reinforcement Learning in Acme'
date: 2021-07-12
---

Introduction
======

Reinforcement learning is one of the pilars of machine learning, next to unsupervised and supervised learning. In 
reinforcement learning you have an agent and an envvironment. The goal of the agent is to learn to pick the best action
given a state the the agent is in in the environment. Globally this environment can be seen as in the following diagram

<figure>
<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/500px-Reinforcement_learning_diagram.svg.png'>
<figcaption>Image taken from Wikipedia</figcaption>
</figure>

The agent performs an action given a state that has been observed and retrieves a reward signal. Based on this reward
signal the agent tries to learn the optimal mapping. Note that this type of learning is somewhere between unsupervised 
and supervised learning. The agent can retrieve "labels" in the form of a reward but these are only implicitely observed
from the environment. This type of learning is much closer to how we as humans for example learn. Thanks goes out to the 
organizers of EEML and Google Deepmind for hosting the EEML 2021 summer school.

the goal of this blog is to implement a simple agent learning to navigate to a goal state using the Acne library. Especially
we will focus on \textbf{value based methods} where the agent has some kind of internel state the remenbers all the state 
action pairs it has seen. 

An action pair is given by a action-value function 

$$
Q^\pi (s, a ) = E_{\tau \approx P^\pi} \sum_t \gamme^t R_t | s_0 = s, a_0 = a
$$

where $\tau$ is the set of all states, actions, and rewards retrieved, i.e. $\tau = {s_0, a_0, r_0, ..., s_n, a_n, r_n}$,
$\pi$ is a given policy that the agent is following. What the equation states is that given a initial state and action and all 
the other visited states and actions the ending value for the table is the sum of the rewards multiplied by $\gamma$. Here 
$\gamma$ is a discount factor for rewards obtained further away from the current state. 

It can be proven that these value estimators can be calculated using the famoud Bellman equation

$$
Q^\pi (s, a ) = r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^\pi(s')
$$

Where $V^\pi(s')$ is the expected value of a given state $s'$. Note that his definition is recursive since the value of 
state s1 and a0 are dependent on the value of s2 and all states given all other actions. 

*Temporal difference* is the difference between the actual action value for a given state and action and the true value that 
you would receive using a specific action in that state.  


Policy iteration is a method where you first evaluate a given policy on your problem. Subsequently you calculate the temporal
difference of that given policy and you change it slightly by this loss. You then *greedy* set the optimal action given a state
to be the action that gives the highest value from that state.

Problem with these approaches is that you are required to fit all the state action pairs into  memory. To account for this 
we would like to just calculate the function values, not all the actual scores. This can be done using *function approximation*
In this blog we look at Neural Fitted Q iteration. So what you basically do is that you represent $Q(s,a )$ using a neural network
which has as the input a state s and outputs Q-values for the possible actions 

Next up is a method where you train a deep neural network to go from a complex state s to actions. In tha atari example
you can for example use a convolutional neural network. 