---
title: 'Paper summaries: Graph Neural Networks'
date: 2021-14-07
---

Introduction
======

For the EEML summer school we had a journal reading group. I especially participated in the group regarding graph neural
networks. In this blog post I write my own summary about the 6 papers we had to read and add some extra info. 

Neural Relation Inference for Interacting Systems
========

Veel systemen in de buitenwereld kan je zien als groepen die met elkaar interactie hebben. Deze interacties zijn moeilijk 
te modelleren want dan zou je per datapunt die interactie moeten opslaan. In deze paper wordt een Neural Relational Model
geintroduceerd zodat deze interacties gemodeleerd kunnen worden in graph neural networks.

In de graph neural network kan je de nodes zien als de verschillende instanties die met elkaar te maken hebben. de impliciete
interacties die dan tussen de instanties worden gedaan kan je zien als de message passing tussen de netwerken. In deze paper
wordt afgestapt van het impliciet modelleren van de iteracties en worden deze interacties specifiek gemaakt. De hoofdvraag
van dit artikel is dhoe je precies die expliciete interactie kan leren. 

Het verschil met het vorige research is dat er nu expliciet relaties worden gelegd tussen de entiteiten met het gebruik
van discrete latente graphs. Ze tonen expliciet aan dat op een NBA dataset ze erg goed instaat zijn om dynamiek tussen 
spelers te modelleren zonder dat je echt veel stappen in de toekomst kijkt. Expliciet vermelden dat ze hier een messag-passing 
netwerk gebruiken. 

De architectuur die wordt voorgesteld is eigenlijk een soort encoder decoder systeem. hier moeten we nog iets zeggen over
dat het een graph VAE is waar je de input x mee stuurt samen met de adjacancy matrix en dje matrix probeer je dan te reconstrueren. In de encoder probeert het systeem
een graph edges te leren vanuit de input trajectories. Een groot verschil is dus dat de edges niet zijn voor het leren van 
de representaties maar je echt de edges wilt leren. 

De distributies q(z|x) geeft dus een latent space waar de edges uit gesampled kunnen worden. Aangezien de output categoriaal
is is het belangrijk dat de gumbel-softmax reparam wordt gebruikt voor het vinden van one-hot encodings. 

Het doel van de decoder is om toekomstige trajectories te voorspellen op basis van de graph en alle trajectories tot xt
Als je vanuit het Markovian principe gaat dan kan je dit direct modelleren. Probleem is dat je bij voor dingen, zoals dat NBA 
verhaal je langere dependencies hebt. Deze worden gemodelleerd door een GRU. 

training gaat als volgd: eerst wordt een input door een encoder gehaald. Vervolgens wordt de gumbel softmax gebruikt om 
categoriale output te halen om te zetten wat de edges worden in de graph. Die graph en de steps worden door de decoder ge
bruikt om voorspellingen te doen wat de output zou zijn.  

Graph Attention Networks
========
De paper begint in een introductie met de observatie dat CNNs het goed doen in problemen die een reguliere grid-like
structuur hebben. In veel gevallen is dat echter niet zo, zoals in social networks. Dit kan worden opgelost met GNNs.
Wat GNNs voornamelijk doen is het doorgeven van infromatie tussen nodes tot een equillibrium is gevonden. vervolgens wordt
er een neuraal netwerk gebruikt om een output te creeren per node. 

Er zijn al een heleboel methodes ontwikkeld voor het toepassen van filters op graphs. Een probleem met deze approach is dat
de filters altijd afhankelijk zijn van de laplacian eigenbasis van de hele graph. Als negatief resultaat heeft dit dat 
je een getrainde graph alleen kunt gebruiken op een graph met dezelfde structuur. Dit valt onder spectral methodes. Spectral
methodes diagonalizeren een matrix. 

Je hebt ook convoluties die direct op graphs worden gebruikt. Een goed voorbeeld hiervan is GraphSage, een algoritme dat
de aggregratie van een node alleen op basis van zijn neighbors doet. 

Als ik het goed begrijp is de enige nieuwigheid het feit dat ze een soort attentie mechanisme gebruiken om de neighbors 
te aggregeren zodat je een nieuwe hidden layer krijgt. Tijdens journal club wordt het vast duidelijker wat het moet zijn


How Attentive are Graph Attention Networks
========

Deze bouwt voort op de vorige paper maar tonen aan dat de attention van GAT eigenlijk niet zo goed is. Het probleem is 
 dat eigenlijk alle attentie naar 1 key ging en dus statisch was. Schijnbaar lossen
ze dit op door een soort dynamische GAT te maken. 