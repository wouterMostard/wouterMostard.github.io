---
title: 'Multilabel classification with PyTorch (Work in Progress)'
date: 2021-04-15
---

Introduction
======
Classificatie is een van de meest voorkomende problemen in machine learning. Meestal gaat classificatie over het voorspellen
tot welke class iets behoort gegeven een input vector, bijvoorbeeld of iemand gaat defaulten op z'n credit card of welk 
team de voetbalwedstrijd gaat winnen. In de natuur echter is een classificatie echter meestal niet los staand van elkaar 
en hebben veel verschillende classificaties overlap met elkaar.

[x] heeft een goed voorbeeld dat betrekking heeft op het classificeren van de leeftijd en de gender van een persoon. Deze
losse classificaties hebben duidelijk met elkaar te maken, zo is een terugtrekkende haarlijn een specifieke kenmerk voor een 
ouder wordende man. De algemene hypothese is dat door informatie van verschillende labels te gebruiken een machine learning
algoritme de relaties kan detecteren tussen deze labels en dus ook een betere classificatie kan opleveren. 

In deze blog post ga ik laten zien hoe je multilabel classificatie kan doen met behulp van PyTorch

Multilabel classification 
======

zoals in de introductie al besproken gaat traditionele classificatie over het classificeren van input $X$ naar een target class
$y$ doormiddel van het leren van een functie $f(x)$, i.e. $f(x) \approx y$. Om specifiek te zijn, we willen de waarschijnlijkheid
hebben dat een label tot class C behoort. In het binaire geval kunnen we dit doen door in de laatste layer van onze neurale 
netwerk een sigmoid functie toe te voegen. Deze functie is gedefinieerd als $\frac{1}{1 + e^{-x}}$. Zoals je in de grafiek
hieronder kunt zien gaat deze functie tussen 0 en 1 

[afbeelding sigmoid]

Note dat de sum van de 2 uitkomsten netjes 1 is, waardoor de sigmoid activatie dus een valide PDF geeft. 
Wanneer je niet een binaire classificatie wil doen maar een classificatie tussen K classes plaats je op het einde een 
softmax activatie. De softmax functie is gedefinieerd als plt $\frac{e^x}{\sum e^{-x}}$


[afbeelding softmax]


Results 
======

Table 2 shows the P@K scores for three text document collections. The scores for the continuous representations are 
shown in the Cont. column. We make several interesting observations. First, the autoencoder with the semantic preservation
 loss performed best in all but two retrieval tasks at various bit sizes. Furthermore, our model performed statistically 
 significant better for > 0.05 compared to the best performing baseline method at each individual task. Second, 
 for the best performing 256-bit semantic preserving autoencoder the degradation with respect to the continuous 
 representations for the 20 newsgroup and Reuters dataset is 5% and 2.7% respectively. Interestingly, the 256-bit
  semantic preserving model has a 4.2% higher P@K score over the continuous representations for the AG news dataset. 
  A similar phenomenon has been observed for the STS14 dataset in Table 1. Third, the semantic preserving model performs 
  better than the vanilla Siamese autoencoder in all but two retrieval tasks and is statistically signiccant for > 0.05 
  for three of the nine tasks.
  
  <img src="http://woutermostard.github.io/files/document_classification.png" align="middle" width="500" height="500">
  
  
 Finally, we were interested into determining whether our Siamese autoencoder learned semantic codewords in the 
 codebook decoder. To determine this we retrieved the most activated bit from the 10 nearest neighbors from each 
 cluster prototype. Since additive vector quantization is used to decode the binary representation the index of the 
 most activated bit corresponds to the most activated code word in the decoder. Table 4 shows the 10 semantically most 
 similar words given the highest activated bit for each cluster prototype.
 
   <img src="http://woutermostard.github.io/files/interpretability.png" align="middle" width="500" height="500">
 
 Some interesting information can be deducted from these code words. First, we see that the fourth bit is frequently
  activated for the Science cluster which seems to be about IT technology. The 20th bit seems to detect verbs and 
  adjectives that would frequently be used in political discussions. The ï¿¿rst bit seems to be detecting names, which
   would often occur in a sports article. Bit number six seems to be about unionization of the work force. Some 
   irrelevant terms, such as paleo and nachos, are also nearest neighbors of the sixth bit. The nearest neighbors of
    the codewords associated with the most activated bits seem relevant for the nearest neighbor of the cluster 
    prototypes given in Table 3. For example, document 1824 seems to be about labor and industry, which is strongly 
    activated with the bit as shown by words such as unionism and Unionists.