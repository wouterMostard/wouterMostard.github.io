---
title: 'Cross-Lingual web information retrieval'
date: 2020-02-14
---

Introduction
======
Lets say you are a Dutchman looking for websites about bikes in the city of London. Considering English is the most 
commonly used language in the city of London you would need to know some <b>keywords</b> that describe bike shops in 
England, e.g. that <i>fiets</i> translates to <i>bicycle</i> and <i>ventiel</i> to <i>valve</i>. To automate this process
is called <b>cross-lingual information retrieval</b> (CLIR). In this post I will describe various method for applying CLIR
and their shortcomings.
 <p>
The internet is a vast source of information written in numerous languages. Structuring these web pages, or documents, 
and quickly returning the most relevant documents is called <b>web information retrieval</b>. Especially in the web domain 
speed is of vital importance considering the size of the internet. A widely used method for storing these documents in
an efficient manner is called the <b>inverted index</b>. Instead of storing each document as a term in a database an
inverted index stores each individual piece of content of a document as a <b>key</b> in a hash map and all the documents that hold
this piece of content as the <b>values</b> of that key. Because the entire index is generated <i>before</i> retrieval 
documents can be retrieved using only the hash table of the individual words in $O(1)$. One of the most used search 
engines for full document text seacrch <i>Elasticsearch</i> uses exactly this method. Below is an example taken
from the excellent book <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html">Introduction to Information Retrieval</a>

<p>
<img src="http://woutermostard.github.io/files/inverted_index.png">
</p>

Notice how the exact terms occurring in both sentences are stored as the index and the corresponding documents as its values.
Finding relevant documents now boils down to finding the index of each query word and return the intersection of the set of
all the returned values as possible matching documents. Although this is a <i>very</i> fast approach for finding documents
that share the <b>same</b> words this is problematic in cross-lingual web information retrieval. Some of them are stated as follows:

<ul>
<li> Queries that contain <b>out-of-vocabulary</b> words, i.e. words that were not encountered during indexing, will not 
be able to help in finding relevant documents considering</li>
<li> For cross-lingual information retrieval it would be necessary to maintain a separate index for each of $N$ the used languages
and $N- 1$ translation tables that map each term in language $l_1$ to the right term in $l_2$. Considering the number 
of languages used on the internet this would be untractable.</li>
</ul>

In conclusie gaat dit over het schrijven van nieuwe mogelijkheden voor het beschrijven van huidige methodes en voorstellen
van nieuwe methodes voor het doen van cross-lingual information retrieval. 

Mogelijke oplossingen 
======
De meest voor de hand liggende oplossing zou zijn het vertalen van elke document in de set van talen naar een doel taal.
Gezien de schaalbare APIs zoals Google Cloud translate is het tegenwoordig mogelijk om op een schaalbare manier veel 
documenten te vertalen. Overduidelijke nadelen zijn het afhankelijk zijn van een externe vertaling mechanisme en een 
mogelijke explosie van kosten over tijd. Gezien de schaal van het web is dit qua kosten niet een reele oplossing. 
<p>
Voor het uitvoeren van cross-lingual information retrieval is het beheren van een verschillende index per taal een tijdrovend
en vatbaar voor fouten, bijvoobeeld door het foutief gebruiken van synonymen. Verder zouden termen die wel in de ene taal
maar niet in de andere taal voorkomen voor mogelijke insufficiente resultaten kunnen leveren.   
</p>
<p>
Een deel van de oplossing zou zijn om word embeddings te gebruiken. In plaats van een sparse matrix te gebruiken waarin
elk woord een aparte dimensie heeft wordt de semantiek van een woord geperst in de lagere dimensionale embedding space.
Deze vorm, ook wel distributed semantics genoemd, biedt een mogelijkheid om documenten te reprsenteren in een lagere 
dimensie terwijl je wel de betekenis houdt. 
</p>
<p>
Een van de grote voordelen van het gebruik van word embeddings is dat het is aangetoond dat los van de lineare relatie
die bestaat tussen woorden in dezelfde taal deze ook vaak voorkomt tussen andere woorden in de andere taal. Met SVD is 
het mogelijk om een lineare transformatie te maken tussen een kleine subset van directe vertalingen. Deze woorden worden
vervolgens gebruikt om elk ander document via een unsupervised manier te vertalen naar de target language.
</p>

Baseline 
======

Results 
======