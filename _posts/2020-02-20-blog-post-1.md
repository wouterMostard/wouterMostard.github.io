---
title: 'Semantic preserving quantization of word embeddings'
date: 2020-02-14
---

Introduction
======
Dense vector representations of words, or word embeddings, have been applied to a wide range of downstream natural language
processing and information retrieval tasks. These word vectors are learned from the co-occurrence of word representations 
in large text corpora using a single layer neural network. After training the word vectors hold syntactic and semantic information about words.

Using continuous representations in large-scale information retrieval tasks, such as web information retrieval, 
has two important disadvantages. First, the continuous representations have significant memory requirements since each
element of the vector is represented as a floating-point number. As a result, fewer documents can be held in main memory. 
Second, comparing continuous vectors requires floating-point similarity measures, such as cosine similarity or dot-product.

 These are more expensive than for example calculating the Hamming distance. Both disadvantages significantly hinder 
 the applicability of continuous representations in large scale information retrieval tasks. 
 A popular approach for decreasing the computational requirements while retaining semantic information is called semantic hashing

Semantic preserving quantization
======

One  solution for this problem is to preserve input topology by introducing a novel hashing method that directly enforces 
retainment of semantic information through a semantics-preserving loss function. Our research focussed on developing a 
Siamese autoencoder architecture that directly tried to optimise the distance between words in the input and latent
space. Our hypothesis was that directly enforcing the semantic similarity in latent space would increase the generated
codes. 



Results 
======

Table 2 shows the P@K scores for three text document collections. The scores for the continuous representations are 
shown in the Cont. column. We make several interesting observations. First, the autoencoder with the semantic preservation
 loss performed best in all but two retrieval tasks at various bit sizes. Furthermore, our model performed statistically 
 significant better for > 0.05 compared to the best performing baseline method at each individual task. Second, 
 for the best performing 256-bit semantic preserving autoencoder the degradation with respect to the continuous 
 representations for the 20 newsgroup and Reuters dataset is 5% and 2.7% respectively. Interestingly, the 256-bit
  semantic preserving model has a 4.2% higher P@K score over the continuous representations for the AG news dataset. 
  A similar phenomenon has been observed for the STS14 dataset in Table 1. Third, the semantic preserving model performs 
  better than the vanilla Siamese autoencoder in all but two retrieval tasks and is statistically signiccant for > 0.05 
  for three of the nine tasks.