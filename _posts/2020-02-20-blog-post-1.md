---
title: 'Semantic preserving quantization of word embeddings'
date: 2020-02-14
---

Introduction
======
Dense vector representations of words, or word embeddings, have been applied to a wide range of downstream natural language
processing and information retrieval tasks. These word vectors are learned from the co-occurrence of word representations 
in large text corpora using a single layer neural network. After training the word vectors hold syntactic and semantic information about words.

Using continuous representations in large-scale information retrieval tasks, such as web information retrieval, 
has two important disadvantages. First, the continuous representations have significant memory requirements since each
element of the vector is represented as a floating-point number. As a result, fewer documents can be held in main memory. 
Second, comparing continuous vectors requires floating-point similarity measures, such as cosine similarity or dot-product.

 These are more expensive than for example calculating the Hamming distance. Both disadvantages significantly hinder 
 the applicability of continuous representations in large scale information retrieval tasks. 
 A popular approach for decreasing the computational requirements while retaining semantic information is called semantic hashing

Semantic preserving quantization
======

One  solution for this problem is to preserve input topology by introducing a novel hashing method that directly enforces 
retainment of semantic information through a semantics-preserving loss function. Our research focussed on developing a 
Siamese autoencoder architecture that directly tried to optimise the distance between words in the input and latent
space. Our hypothesis was that directly enforcing the semantic similarity in latent space would increase the generated
codes. 



Results 
======

