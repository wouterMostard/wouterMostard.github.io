---
title: 'Multilabel classification with PyTorch'
date: 2021-04-15
---

Introduction
======
Classification is one of the most well studied problems in machine learning. Usually this classification is based
on some input representation and the goal of the machine learning model is to predict which of $K$ target classes this example 
belongs to. Examples are predicting whether a credit card customer will default or object classification in computer vision.
Although practical for many classification problems, there is one distinct problem with this proposition of classification. 

Most objects in nature do not belong to a single class. In the object classification task a target object could be 
classified as a dog, but that would also make it a mammal. Not only is posing the problem of classification to a single target class too restrictive,
you also lose potentially viable information that can be used in classification. 

[Levi et al] showed that classification of a person's age benefits from trying to classify the gender of that person at the same time.
 For example, having a receding hairline is not only a good indicator for age but also increases the probability of the 
 person being a male. The main hypothesis is that classification can be improved by combining information from the posed target
 classes and performing multiple classifications at the same time.  

In this post I will give a generic introduction on how multi-label classification problems can be posed in PyTorch.   

Multi-label classification 
======

As already discussed in the introduction traditional classification problems are given as transforming an input vector
$X$ into a prediction $y$ by learning a function $f(X) \approx y$. In this example we will use a simple neural network as our 
function $f(x)$. We will be using a probabilistic approach to classification where we are modelling the probability of an input vector belonging to class $C$. The target label is selected by
choosing the class with the highest posterior probability, i.e. $max_c P(c | x)$.

When performing binary classification the last layer of a neural network is usually a single node with a Sigmoid activation function.
The Sigmoid function transforms the output of the last latent layer, also called logits, between 0 and 1 and is given by $\frac{1}{1 + e^{-x}}$.
Below if an example where the output of the last layer between -5 and 5 is transformed. As you can see the output is nicely between
0 and 1. This models the probability of class 1, i.e. $P(C_1|X)$. Since this is binary classification the probability of $C_2$
is then given by $P(C_2|X) = 1 - P(C_1|X)$. Both probabilitis are between 0 and 1 and their sum is 1, making this a 
valid probability.  

  <img src="http://woutermostard.github.io/files/sigmoid.png" align="middle" width="500" height="500">

When you are trying to classify one of $K$ classes you can let the last layer consist of $K$ nodes and apply a Softmax 
function. The softmax function is defined as $\frac{e^x}{\sum e^{x}}$ and can be interpreted as a generalisation of the 
sigmoid function to $K$ classes. For example, see the figure below where we transformed the output of the last layer, [1.1, 3.0, 1.3, 1.4], using the Softmax
equation.

  <img src="http://woutermostard.github.io/files/softmax.png" align="middle" width="500" height="500">

Just as with the Sigmoid function all results are between 0 and 1 and the sum of all the nodes is 1 and thus the result
can be interpreted as a valid probability density function. 

Both examples given above assume that $y$ is a single value. How can we extend this idea to multi-label classification 
where $y$ is given as a vector. For this we need a multinomial, i.e. having more than 1 term, solution.
In statistics there exists multinomial logistic regression which expands the Sigmoid function given above to account
for multiple target labels. Now with some required background information we can try to implement this in PyTorch.        

Implementation in PyTorch 
======

To implement our multinomial model we create a simple model with 1 hidden layer as shown below. 

    class Classifier(nn.Module):
        def __init__(self, input_shape, n_classes, hidden_size):
            super().__init__()
            self.input_shape = input_shape
            self.n_classes = n_classes
            self.hidden_size = hidden_size
    
            self.mlp = nn.Sequential(
                nn.Linear(self.input_shape, self.hidden_size),
                nn.ReLU(),
                nn.Linear(self.hidden_size, self.n_classes)
            )
    
        def forward(self, x):
            return self.mlp(x)

Note that the architecture of the model, given in `self.mlp`, is a simple multilayered perceptron that first linearly transforms
the input into a latent representation, applies the ReLU activation, and last again linearly transforms the data but now 
into a space where the number of output nodes matches the number of classes. Note that no activation is performed in the last 
layer. This is for computational reasons and is described below.

With the model in place we require some input data. For this simple example we will use artificial data 
using the `sklearn.datasets` module.

    from sklearn.datasets import make_multilabel_classification
    
    X, y = make_multilabel_classification(n_samples=800, n_classes=5)
    print(f"Shape of input: {X.shape}")
    Shape of input: (800, 20)
    print(f"Shape of target: {y.shape}")
    Shape of target: (800, 5)
    print(y[:3])
    [[0 0 0 1 0]
     [1 1 0 1 1]
     [1 0 0 1 1]]
     
The function creates a dataset with 800 examples with an feature size of 20. The output $y$ is also a vector
where each index is 1 if the target class exists and 0 otherwise. Note that the number of columns is 5, meaning that there 
are 5 target classes. As you can see for example in row 2 of the output vectors multiple items can be set to 1 in a single vector.

This dataset will be served in batches to train our neural network. Thankfully PyTorch has useful modules that 
do most of the data handling for you so we import these as well. 

    from typing import  Tuple
    
    from torch.utils.data import Dataset, Dataloader
    import numpy as np
    
    class MultiLabelDataset(Dataset):
        def __init__(self, input_data: np.ndarray, y: np.ndarray):
            self.X: np.ndarray = input_data
            self.y: np.ndarray = y
    
        def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:
            return self.X[idx, :], self.y[idx, :]
    
        def __len__(self) -> int:
            return self.X.shape[0]
            
Here we created a custom dataset class where we specify 2 special method names, `__len__` and `__getitem__`. THe first one
is required for specifying the size of the dataset (such that `len(dataset)`) and `__getitem__` allows for indexing on the 
dataset, i.e. `dataset[idx]`. With all these things in place we are ready to train our model. 

    model = Classifier(X.shape[1], y.shape[1], 10)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    dataset = MultiLabelDataset(x_train, y_train)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
    lossfunc = nn.BCEWithLogitsLoss()
    
    losses = []
    
    for epoch in range(2000):
        for X_batch, y_batch in dataloader:
            optimizer.zero_grad()
            prediction = model(X_batch.float())
            loss = lossfunc(input=prediction, target=y_batch.float())
    
            loss.backward()
            losses.append(loss.item())
    
            optimizer.step()
            
Note that we use the `BCEWithLogitsLoss` and not the Binary cross entropy loss as you would expect. This 
is the case bacause using the log sum trick. The model calculates a point-wise sigmoid activation, meaning that for each
 output node the score is transformed between 0 and 1, but that the sum of the output layer could very well be higher 
 than 1, with a maximum of an activation of 1 for each output node. This is a more stable solution than using a sigmoid function at the 
last layer and then calculating the binary cross entropy. To evaluate on a test set we can use the torch.sigmoid function 
to create probabilities out of the logits 

    from sklearn.metrics import classification_report
    
    predictions = (torch.sigmoid(model(torch.from_numpy(x_test).float())) > 0.5).detach().numpy().astype(int)    
    print(classification_report(y_pred=predictions, y_true=y_test))
    
                      precision    recall  f1-score   support
    
               0       0.86      0.90      0.88       130
               1       0.40      0.09      0.14        23
               2       0.75      0.14      0.24        21
               3       0.81      0.66      0.73        88
               4       0.76      0.78      0.77       104
    
       micro avg       0.81      0.71      0.76       366
       macro avg       0.72      0.51      0.55       366
    weighted avg       0.78      0.71      0.73       366
     samples avg       0.76      0.68      0.68       366
     

Citations 
======
Gil Levi and Tal Hassner - <em>Age and Gender Classification using Convolutional Neural Networks</em>. CVPR 2015

