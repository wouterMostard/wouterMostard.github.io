---
title: 'Multilabel classification with PyTorch'
date: 2021-04-15
---

Introduction
======
Classification is one of the most frequently occurring problems in machine learning. Usually this classification is based
one some input vector and the goal of the machine learning model is to predict on of $K$ target classes. Examples are 
predicting whether a credit card customer will default or object classification in computer vision. There is one distinct
problem with this proposition of classification. 

Most objects in nature doe not belong to a single class. In the object classification task a target object could be 
a dog but that also makes it a mammal. Not only is posing the problem of classification to a single target class too restrictive,
you also lose potentially viable information that can be used in classification. 

[x] showed that classification of a person's age benefits from trying to classify the gender of that person at the same time.
 For example, having a receding hairline is not only a good indicator for the age but also increases the probability of the 
 person being a male. The main hypothesis is that classification can be improved by combining information from the posed target
 classes in the forward pass through a neural networks.  

In this post I will give a generic introduction on how multilabel classification problems can be posed in PyTorch.   

Multilabel classification 
======

As already discussed in the introduction traditional classification problems are posed as transforming an input vector
$X$ into a prediction $y$ by learning a function $f(X) \approx y$. Since we will be using PyTorch we use a probabilitistic
approach where we are modelling the probability of an input vector belonging to class $C$. The target label is selected by
choosing the class with the highest posterior probability, i.e. $max_c P(c | x)$.

When performing binary classification the last layer of a neural network is usually a single node with a sigmoid activation layer
The sigmoid function squuashes the output of the last latent layer between 0 and 1 and is given by $\frac{1}{1 + e^{-x}}$.
Below if an example where the output of the last layer between -5 and 5 is transformed. As you can see the is nicely between
0 and 1, i.e. it is a valid probability density function, so this result can be interpreted as a probability. 

  <img src="http://woutermostard.github.io/files/sigmoid.png" align="middle" width="500" height="500">

When you are trying to classify one of $K$ classes you can let the last layer consist of $K$ nodes and apply a softmax 
activation. The softmax function is defined as $\frac{e^x}{\sum e^{x}}$ and can be interpreted as a smooth version of argmax. 
For example, see the figure below where we transformed the output of the last layer, [1.1, 3.0, 1.3, 1.4], using the softmax
equation.

  <img src="http://woutermostard.github.io/files/softmax.png" align="middle" width="500" height="500">

Just as with the logistic function all results are between 0 and 1 and the sum of all the nodes is 1 and thus the result
can be intrepreted as a valid probability density function. 

Both examples given above assume that $y$ is a single value. How can we extend this idea to multiclass classification 
where $y$ is given as a list of multiple valid answers. For this we need a multinomial, i.e. having more than 1 term, solution.
In statistics there exists multinomial logistic regression which expands the sigmoidal function given above to account
for multiple target labels. Now with some required background information we can try to implement this in PyTorch.        

Implementation in PyTorch 
======

To implement our multinomial model we create a simple model with 1 hidden layer as shown below. 

    class Classifier(nn.Module):
        def __init__(self, input_shape, n_classes, hidden_size):
            super().__init__()
            self.input_shape = input_shape
            self.n_classes = n_classes
            self.hidden_size = hidden_size
    
            self.mlp = nn.Sequential(
                nn.Linear(self.input_shape, self.hidden_size),
                nn.ReLU(),
                nn.Linear(self.hidden_size, self.n_classes)
            )
    
        def forward(self, x):
            return self.mlp(x)

Note that the output of the model given in `self.mlp` is a simple multilayered perceptron that first linearly transforms
the input into a latent representation, applies the ReLU activation, and last again linearly transforms the data but now 
into a space where the number of output nodes matches the number of classes. Note that no activation is performed in the last 
layer. 

With the model in place we require some input data. For this priming example we will use artificial data 
using the sklearn datasets module. 

    from sklearn.datasets import make_multilabel_classification
    
    X, y = make_multilabel_classification(n_samples=800, n_classes=5)
    print(f"Shape of input: {X.shape}")
    Shape of input: (800, 20)
    print(f"Shape of target: {y.shape}")
    Shape of target: (800, 5)
    print(y[:3])
    [[0 0 0 1 0]
     [1 1 0 1 1]
     [1 0 0 1 1]]
     
The function created a dataset with 800 examples with an input feature size of 20. The output $y$ is also a vector
where each index is 1 of the target class exists and 0 otherwise. Note that the number of columns is 5, meaning that there 
are 5 target classes. As you can see in row 2 of the output vectors multiple items can be set to 1 in a single vector.

This dataset will be served in batches to our neural network. Thankfully Torch has useful modules that 
do most of the data handling for you so we import these as well. 

    from typing import  Tuple
    
    from torch.utils.data import Dataset, Dataloader
    import numpy as np
    
    class MultiLabelDataset(Dataset):
        def __init__(self, input_data: np.ndarray, y: np.ndarray):
            self.X: np.ndarray = input_data
            self.y: np.ndarray = y
    
        def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:
            return self.X[idx, :], self.y[idx, :]
    
        def __len__(self) -> int:
            return self.X.shape[0]
            
Here we created a custom dataset class where we specify how the library can determine the size of the dataset
and another method that described how items are selected from the dataset. 

With all these things in place we are ready to train our model. 

    model = Classifier(X.shape[1], y.shape[1], 10)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    dataset = MultiLabelDataset(x_train, y_train)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
    lossfunc = nn.BCEWithLogitsLoss()
    
    losses = []
    
    for epoch in range(2000):
        for X_batch, y_batch in dataloader:
            optimizer.zero_grad()
            prediction = model(X_batch.float())
            loss = lossfunc(input=prediction, target=y_batch.float())
    
            loss.backward()
            losses.append(loss.item())
    
            optimizer.step()
            
Note that we use the BCEWithLogitsLoss and not the Binary cross entropy loss as you would expect. This 
is the case bacause using the log sum trick. The model calculates a point-wise sigmoid activation, meaning that for each
 output node the score is transformed between 0 and 1, but that the sum of the output layer could very well be higher 
 than 1, with a maximum of an activation of 1 for each output node. This is a more stable solution than using a sigmoid function at the 
last layer and then calculating the binary cross entropy. To evaluate on a test set we can use the torch.sigmoid function 
to create probabilities out of the logits 

    from sklearn.metrics import classification_report
    
    predictions = (torch.sigmoid(model(torch.from_numpy(x_test).float())) > 0.5).detach().numpy().astype(int)    
    print(classification_report(y_pred=predictions, y_true=y_test))
    
                      precision    recall  f1-score   support
    
               0       0.86      0.90      0.88       130
               1       0.40      0.09      0.14        23
               2       0.75      0.14      0.24        21
               3       0.81      0.66      0.73        88
               4       0.76      0.78      0.77       104
    
       micro avg       0.81      0.71      0.76       366
       macro avg       0.72      0.51      0.55       366
    weighted avg       0.78      0.71      0.73       366
     samples avg       0.76      0.68      0.68       366
     

