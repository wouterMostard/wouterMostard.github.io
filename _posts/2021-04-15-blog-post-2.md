---
title: 'Multilabel classification with PyTorch (Work in Progress)'
date: 2021-04-15
---

Introduction
======
Classificatie is een van de meest voorkomende problemen in machine learning. Meestal gaat classificatie over het voorspellen
tot welke class iets behoort gegeven een input vector, bijvoorbeeld of iemand gaat defaulten op z'n credit card of welk 
team de voetbalwedstrijd gaat winnen. In de natuur echter is een classificatie echter meestal niet los staand van elkaar 
en hebben veel verschillende classificaties overlap met elkaar.

[x] heeft een goed voorbeeld dat betrekking heeft op het classificeren van de leeftijd en de gender van een persoon. Deze
losse classificaties hebben duidelijk met elkaar te maken, zo is een terugtrekkende haarlijn een specifieke kenmerk voor een 
ouder wordende man. De algemene hypothese is dat door informatie van verschillende labels te gebruiken een machine learning
algoritme de relaties kan detecteren tussen deze labels en dus ook een betere classificatie kan opleveren. 

In deze blog post ga ik laten zien hoe je multilabel classificatie kan doen met behulp van PyTorch

Multilabel classification 
======

zoals in de introductie al besproken gaat traditionele classificatie over het classificeren van input $X$ naar een target class
$y$ doormiddel van het leren van een functie $f(x)$, i.e. $f(x) \approx y$. Om specifiek te zijn, we willen de waarschijnlijkheid
hebben dat een label tot class C behoort. In het binaire geval kunnen we dit doen door in de laatste layer van onze neurale 
netwerk een sigmoid functie toe te voegen. Deze functie is gedefinieerd als $\frac{1}{1 + e^{-x}}$. Zoals je in de grafiek
hieronder kunt zien gaat deze functie tussen 0 en 1 

  <img src="http://woutermostard.github.io/files/sigmoid.png" align="middle" width="500" height="500">

Note dat de sum van de 2 uitkomsten netjes 1 is, waardoor de sigmoid activatie dus een valide PDF geeft. 
Wanneer je niet een binaire classificatie wil doen maar een classificatie tussen K classes plaats je op het einde een 
softmax activatie. De softmax functie is gedefinieerd als plt $\frac{e^x}{\sum e^{-x}}$. Hieronder staat een afbeelding 
waarbij de outputs van de laatste lineare layer [1.1, 3.0, 1.3, 1.4] waren.


  <img src="http://woutermostard.github.io/files/softmax.png" align="middle" width="500" height="500">


Ook hier geldt dat de som van alle waardes van de laatste layer netjes uitkomt op 1 waardoor je het als een valide PDf kan zien. 
Deze functies heb je waarschijnlijk als enige keren voorbij zien komen en je vraagt je misschien af waarom deze functies n
nogmaals worden besproken. 

Zoals al eerder besproken willen we bij multilabel classification dus voor een gegeven X weten wat de y is. De boven 
genoemde functies zullen hier helaas niet mee helpen want ze bieden aleen de mogelijkheid om te optimaliseren tot 1 target.
Om classfiicaties te doen waar meerdere labels "waar" kunnen zijn hebben we een multinomial oplossing nodig. binnen de 
statistiek wordt dit dus dan ook multinomial logistic regression genoemd, i.e. we voeren logistische regressie meerdere malen
uit gegeven een input vector. 


Implementation in PyTorch 
======

Om dit te implementeren in PyTorch gebruiken we een simpel voorbeeld van een Multi Layered Perceptron met e
een enkele linear layer. 

    class Classifier(nn.Module):
        def __init__(self, input_shape, n_classes, hidden_size):
            super().__init__()
            self.input_shape = input_shape
            self.n_classes = n_classes
            self.hidden_size = hidden_size
    
            self.mlp = nn.Sequential(
                nn.Linear(self.input_shape, self.hidden_size),
                nn.ReLU(),
                nn.Linear(self.hidden_size, self.n_classes)
            )
    
        def forward(self, x):
            return self.mlp(x)

Om wat voorbeeld data te maken maken we gebruk van de create datasets van sklearn. 

    from sklearn.datasets import make_multilabel_classification
    
    X, y = make_multilabel_classification()
    
    print(f"Shape of input: {X.shape}")
    Shape of input: (100, 20)
    print(f"Shape of target: {y.shape}")
    Shape of target: (100, 5)
    print(y[:3]
    [[0 0 0 0 0]
    [1 1 0 1 0]
    [0 0 0 0 1]]
    
Er is dus een dataset gemaakt van 100 examples die een input vector heeft van 20 features. De output is 
een vector waarbij elke colom staat voor 1 van de target classes. Zoals je onderin kunt zien kunnen meerdere
colommen 1 zijn, dus inderdaad een multilabel classificatie. Voor het trainen is het handig om een 
Dataset en Dataloader te gebruiken. 

    from typing import  Tuple
    
    from torch.utils.data import Dataset, Dataloader
    import numpy as np
    
    class MultiLabelDataset(Dataset):
        def __init__(self, input_data: np.ndarray, y: np.ndarray):
            self.X: np.ndarray = input_data
            self.y: np.ndarray = y
    
        def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:
            return self.X[idx, :], self.y[idx, :]
    
        def __len__(self) -> int:
            return self.X.shape[0]
            
Then using this dataset and the dataloader class we can train our model

    model = Classifier(X.shape[1], y.shape[1], 10)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    dataset = MultiLabelDataset(x_train, y_train)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
    lossfunc = nn.BCEWithLogitsLoss()
    
    losses = []
    
    for epoch in range(2000):
        for X_batch, y_batch in dataloader:
            optimizer.zero_grad()
            prediction = model(X_batch.float())
            loss = lossfunc(input=prediction, target=y_batch.float())
    
            loss.backward()
            losses.append(loss.item())
    
            optimizer.step()
            
Note that we use the BCEWithLogitsLoss and not the Binary cross entropy loss as you would expect. This 
is the case bacause using the log sum trick. This prevents overfitting and this allows to calculate the 
loss after a linear layer. To evaluate on a test set we can use the torch.sigmoid function to create 
probabilities out of the logits 

    from sklearn.metrics import classification_report
    
    predictions = (torch.sigmoid(model(torch.from_numpy(x_test).float())) > 0.5).detach().numpy().astype(int)    
    print(classification_report(y_pred=predictions, y_true=y_test))
    
                      precision    recall  f1-score   support
    
               0       0.86      0.90      0.88       130
               1       0.40      0.09      0.14        23
               2       0.75      0.14      0.24        21
               3       0.81      0.66      0.73        88
               4       0.76      0.78      0.77       104
    
       micro avg       0.81      0.71      0.76       366
       macro avg       0.72      0.51      0.55       366
    weighted avg       0.78      0.71      0.73       366
     samples avg       0.76      0.68      0.68       366
