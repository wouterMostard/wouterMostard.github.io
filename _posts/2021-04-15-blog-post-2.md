---
title: 'Multilabel classification with PyTorch (Work in Progress)'
date: 2021-04-15
---

Introduction
======
Classification is one of the most frequently occurring problems in machine learning. Usually this classification is based
one some input vector and the goal of the machine learning model is to predict on of $K$ target classes. Examples are 
predicting whether a credit card customer will default or object classification in computer vision. There is one distinct
problem with this proposition of classification. 

Most objects in nature doe not belong to a single class. In the object classification task a target object could be 
a dog but that also makes it a mammal. Not only is posing the problem of classification to a single target class too restrictive,
you also lose potentially viable information that can be used in classification. 

[x] showed that classification of a person's age benefits from trying to classify the gender of that person at the same time.
 For example, having a receding hairline is not only a good indicator for the age but also increases the probability of the 
 person being a male. The main hypothesis is that classification can be improved by combining information from the posed target
 classes in the forward pass through a neural networks. 

[x] heeft een goed voorbeeld dat betrekking heeft op het classificeren van de leeftijd en de gender van een persoon. Deze
losse classificaties hebben duidelijk met elkaar te maken, zo is een terugtrekkende haarlijn een specifieke kenmerk voor een 
ouder wordende man. De algemene hypothese is dat door informatie van verschillende labels te gebruiken een machine learning
algoritme de relaties kan detecteren tussen deze labels en dus ook een betere classificatie kan opleveren. 

In this post I will give a generic introduction on how multilabel classification problems can be posed in PyTorch.   

Multilabel classification 
======

As already discussed in the introduction traditional classification problems are posed as transforming an input vector
$X$ into a prediction $y$ by learning a function $f(X) \approx y$. Since we will be using PyTorch we use a probabilitistic
approach where we are modelling the probability of an input vector belonging to class $C$. The target label is selected by
choosing the class with the highest posterior probability, i.e. $max_c P(c | x)$.

When performing binary classification the last layer of a neural network is usually a single node with a sigmoid activation layer
The sigmoid function squuashes the output of the last latent layer between 0 and 1 and is given by $\frac{1}{1 + e^{-x}}$.
Below if an example where the output of the last layer between -5 and 5 is transformed. As you can see the is nicely between
0 and 1, i.e. it is a valid probability density function, so this result can be interpreted as a probability. 

  <img src="http://woutermostard.github.io/files/sigmoid.png" align="middle" width="500" height="500">

When you are trying to classify one of $K$ classes you can let the last layer consist of $K$ nodes and apply a softmax 
activation. The softmax function is defined as $\frac{e^x}{\sum e^{-x}}$ and can be interpreted as a smooth version of argmax. 
For example, see the figure below where we transformed the output of the last layer, [1.1, 3.0, 1.3, 1.4], using the softmax
equation.

  <img src="http://woutermostard.github.io/files/softmax.png" align="middle" width="500" height="500">

Just as with the logistic function all results are between 0 and 1 and the sum of all the nodes is 1 and thus the result
can be intrepreted as a valid probability density function. 

Both examples given above assume that $y$ is a single value. How can we extend this idea to multiclass classification 
where $y$ is given as a list of multiple valid answers. For this we need a multinomial, i.e. having more than 1 term, solution.
In statistics there exists multinomial logistic regression which expands the sigmoidal function given above to account
for multiple target labels. Now with some required background information we can try to implement this in PyTorch.        

Implementation in PyTorch 
======

Om dit te implementeren in PyTorch gebruiken we een simpel voorbeeld van een Multi Layered Perceptron met e
een enkele linear layer. 

    class Classifier(nn.Module):
        def __init__(self, input_shape, n_classes, hidden_size):
            super().__init__()
            self.input_shape = input_shape
            self.n_classes = n_classes
            self.hidden_size = hidden_size
    
            self.mlp = nn.Sequential(
                nn.Linear(self.input_shape, self.hidden_size),
                nn.ReLU(),
                nn.Linear(self.hidden_size, self.n_classes)
            )
    
        def forward(self, x):
            return self.mlp(x)

Om wat voorbeeld data te maken maken we gebruk van de create datasets van sklearn. 

    from sklearn.datasets import make_multilabel_classification
    
    X, y = make_multilabel_classification(n_samples=800, n_classes=5)
    print(f"Shape of input: {X.shape}")
    Shape of input: (800, 20)
    print(f"Shape of target: {y.shape}")
    Shape of target: (800, 5)
    print(y[:3])
    [[0 0 0 1 0]
     [1 1 0 1 1]
     [1 0 0 1 1]]
    
Er is dus een dataset gemaakt van 800 examples die een input vector heeft van 20 features. De output is 
een vector waarbij elke colom staat voor 1 van de target classes. Zoals je onderin kunt zien kunnen meerdere
colommen 1 zijn, dus inderdaad een multilabel classificatie. Voor het trainen is het handig om een 
Dataset en Dataloader te gebruiken. 

    from typing import  Tuple
    
    from torch.utils.data import Dataset, Dataloader
    import numpy as np
    
    class MultiLabelDataset(Dataset):
        def __init__(self, input_data: np.ndarray, y: np.ndarray):
            self.X: np.ndarray = input_data
            self.y: np.ndarray = y
    
        def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:
            return self.X[idx, :], self.y[idx, :]
    
        def __len__(self) -> int:
            return self.X.shape[0]
            
Then using this dataset and the dataloader class we can train our model

    model = Classifier(X.shape[1], y.shape[1], 10)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    dataset = MultiLabelDataset(x_train, y_train)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
    lossfunc = nn.BCEWithLogitsLoss()
    
    losses = []
    
    for epoch in range(2000):
        for X_batch, y_batch in dataloader:
            optimizer.zero_grad()
            prediction = model(X_batch.float())
            loss = lossfunc(input=prediction, target=y_batch.float())
    
            loss.backward()
            losses.append(loss.item())
    
            optimizer.step()
            
Note that we use the BCEWithLogitsLoss and not the Binary cross entropy loss as you would expect. This 
is the case bacause using the log sum trick. This prevents overfitting and this allows to calculate the 
loss after a linear layer. To evaluate on a test set we can use the torch.sigmoid function to create 
probabilities out of the logits 

    from sklearn.metrics import classification_report
    
    predictions = (torch.sigmoid(model(torch.from_numpy(x_test).float())) > 0.5).detach().numpy().astype(int)    
    print(classification_report(y_pred=predictions, y_true=y_test))
    
                      precision    recall  f1-score   support
    
               0       0.86      0.90      0.88       130
               1       0.40      0.09      0.14        23
               2       0.75      0.14      0.24        21
               3       0.81      0.66      0.73        88
               4       0.76      0.78      0.77       104
    
       micro avg       0.81      0.71      0.76       366
       macro avg       0.72      0.51      0.55       366
    weighted avg       0.78      0.71      0.73       366
     samples avg       0.76      0.68      0.68       366
