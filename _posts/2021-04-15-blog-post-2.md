---
title: 'Multilabel classification with PyTorch (Work in Progress)'
date: 2021-04-15
---

Introduction
======
Classificatie is een van de meest voorkomende problemen in machine learning. Meestal gaat classificatie over het voorspellen
tot welke class iets behoort gegeven een input vector, bijvoorbeeld of iemand gaat defaulten op z'n credit card of welk 
team de voetbalwedstrijd gaat winnen. In de natuur echter is een classificatie echter meestal niet los staand van elkaar 
en hebben veel verschillende classificaties overlap met elkaar.

[x] heeft een goed voorbeeld dat betrekking heeft op het classificeren van de leeftijd en de gender van een persoon. Deze
losse classificaties hebben duidelijk met elkaar te maken, zo is een terugtrekkende haarlijn een specifieke kenmerk voor een 
ouder wordende man. De algemene hypothese is dat door informatie van verschillende labels te gebruiken een machine learning
algoritme de relaties kan detecteren tussen deze labels en dus ook een betere classificatie kan opleveren. 

In deze blog post ga ik laten zien hoe je multilabel classificatie kan doen met behulp van PyTorch

Multilabel classification 
======

zoals in de introductie al besproken gaat traditionele classificatie over het classificeren van input $X$ naar een target class
$y$ doormiddel van het leren van een functie $f(x)$, i.e. $f(x) \approx y$. Om specifiek te zijn, we willen de waarschijnlijkheid
hebben dat een label tot class C behoort. In het binaire geval kunnen we dit doen door in de laatste layer van onze neurale 
netwerk een sigmoid functie toe te voegen. Deze functie is gedefinieerd als $\frac{1}{1 + e^{-x}}$. Zoals je in de grafiek
hieronder kunt zien gaat deze functie tussen 0 en 1 

  <img src="http://woutermostard.github.io/files/sigmoid.png" align="middle" width="500" height="500">

Note dat de sum van de 2 uitkomsten netjes 1 is, waardoor de sigmoid activatie dus een valide PDF geeft. 
Wanneer je niet een binaire classificatie wil doen maar een classificatie tussen K classes plaats je op het einde een 
softmax activatie. De softmax functie is gedefinieerd als plt $\frac{e^x}{\sum e^{-x}}$. Hieronder staat een afbeelding 
waarbij de outputs van de laatste lineare layer [1.1, 3.0, 1.3, 1.4] waren.


  <img src="http://woutermostard.github.io/files/softmax.png" align="middle" width="500" height="500">


Ook hier geldt dat de som van alle waardes van de laatste layer netjes uitkomt op 1 waardoor je het als een valide PDf kan zien. 
Deze functies heb je waarschijnlijk als enige keren voorbij zien komen en je vraagt je misschien af waarom deze functies n
nogmaals worden besproken. 

Zoals al eerder besproken willen we bij multilabel classification dus voor een gegeven X weten wat de y is. De boven 
genoemde functies zullen hier helaas niet mee helpen want ze bieden aleen de mogelijkheid om te optimaliseren tot 1 target.
Om classfiicaties te doen waar meerdere labels "waar" kunnen zijn hebben we een multinomial oplossing nodig. binnen de 
statistiek wordt dit dus dan ook multinomial logistic regression genoemd, i.e. we voeren logistische regressie meerdere malen
uit gegeven een input vector. 


Results 
======
