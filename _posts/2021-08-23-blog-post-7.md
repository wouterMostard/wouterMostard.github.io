---
title: 'WIP: Binary Graph Neural Networks'
date: 2021-07-12
---

Introduction
======

Graph Neural Networks have been applied to situations where the data is not in Euclidean space. Convolutions on graphs
have been applied to a wide range of Euclidean space models already, such as CNNs on images or RNNs on texts. On of the 
problems of the recently developed GNN models is that, just like the CNN models, they have a lot of parameters and do not 
really scale well to be applied on for example mobile devices or on a large scale. Furthermore you have to take into 
account the fact that training models for example on a GPU can be quite expensive and that sometimes you are on a low 
resource environment such as on a mobile phone. In this paper the researchers focus primarely on compressing current models
into a better representation. This is different than other papers where they primarely focus on algorithmic changes such
as sampling and achitectural improvements. 


Related Work
=======

So one of the primary things to look at is how we can quantize the model. This means that we would like to lower the numerical
precision of the model while retaining its performance. In Binary networks we represent each weight value by a single digit. 
One important thing in neural networks is to calculate the dot product between 2 vectors. [x] showed that given 2 real-valued
vectors $a$ and $b$ you can approximate the dot product by

$$
a \star b \approx (sign(a) \circledast sign(b)) \alpha \beta
$$

Where $\alpha$ and $\beta$ are the rescaling constants for $a$ and $b$ respectively. This can be implemented in Python
using the following code:

```
dataset = np.random.uniform(0, 1, (500, 300))
dataset /= np.linalg.norm(dataset, axis=1, keepdims=True)

def sign(vector):
    return 2* (vector > 0).astype(int) - 1

def rescaling(vector):
    return (1 / vector.size) * np.sum(np.abs(vector))

def binary_product(vector_x, vector_y):
    return (vector_x == vector_y).sum()


def calculate_approx(a, b):
    alpha = rescaling(a)
    beta = rescaling(b)

    sign_a = sign(a)
    sign_b = sign(b)

    return binary_product(sign_a, sign_b) * alpha * beta

print(calculate_approx(dataset[0], dataset[1]))
print(dataset[0] @ dataset[1])
```

As you would see it is quite well possible to approximate the dot product of two real-valued vectors using 


