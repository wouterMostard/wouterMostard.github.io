---
title: 'WIP: Introduction into JAX and Haiku'
date: 2021-04-15
---

Introduction
======

JAX is a simple python libary that allows for automatic differentation and optimization of linear algebra optimization. 
Why differentation is required is opbvious, it is needed for passing information back through the network. Using 
XLA (Accelerated Linear Algebra) is maybe a bit less obvious. The important thing to note is that for example a basic 
multilayered perceptron is simple a sequential application of matrix multiplications. XLA allows for optimizateion of 
these linear algebra operations by compiling the model.

In this short blog bost I will show how you can develop a simple regression model on the boston house pricing dataset
using JAX. Furthermore, we will show how Haiku (which is a simple wrapper arround JAX) can be used to develop more 
complicated models. In our  case we will develop a CNN for performing MNIST digit classifciation  
 

Automatic differentiation of simple functions 
======

```
from jax import grad, jit # Load the automatic differentation and just in time compilation
import jax.numpy as jnp # duck typing numpy 
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + jnp.exp(-x))

gradient = grad(sigmoid) # calculates the gradient w.r.t. x
hessian = grad(gradient)

x_range = jnp.linspace(-5, 5, 100)
plt.plot(x_range, [gradient(x) for x in x_range], label='derivative')
plt.plot(x_range, [hessian(x) for x in x_range], label='hessian')
plt.plot(x_range, sigmoid(x_range), label='$\sigma$')
```

<img src="http://woutermostard.github.io/files/differ.png" align="middle" width="500" height="300">



