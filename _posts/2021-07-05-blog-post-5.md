---
title: 'WIP: Introduction into JAX and Haiku'
date: 2021-04-15
---

Introduction
======

JAX is a simple python libary that allows for automatic differentation and optimization of linear algebra optimization. 
Why differentation is required is opbvious, it is needed for passing information back through the network. Using 
XLA (Accelerated Linear Algebra) is maybe a bit less obvious. The important thing to note is that for example a basic 
multilayered perceptron is simple a sequential application of matrix multiplications. XLA allows for optimizateion of 
these linear algebra operations by compiling the model.

In this short blog bost I will show how you can develop a simple regression model on the boston house pricing dataset
using JAX. Furthermore, we will show how Haiku (which is a simple wrapper arround JAX) can be used to develop more 
complicated models. In our  case we will develop a CNN for performing MNIST digit classifciation  
 

Automatic differentiation of simple functions 
======

```
from jax import grad, jit # Load the automatic differentation and just in time compilation
import jax.numpy as jnp # duck typing numpy 
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + jnp.exp(-x))

gradient = grad(sigmoid) # calculates the gradient w.r.t. x
hessian = grad(gradient)

x_range = jnp.linspace(-5, 5, 100)
plt.plot(x_range, [gradient(x) for x in x_range], label='derivative')
plt.plot(x_range, [hessian(x) for x in x_range], label='hessian')
plt.plot(x_range, sigmoid(x_range), label='$\sigma$')
```

<img src="http://woutermostard.github.io/files/differ.png" align="middle" width="500" height="300">

Create simple Logistic regression classifier using JAX 
======

First we create some binary data set for classification of 500 samples, split the data into train and test and scale the
data accordingly. Scaling is required because it could be the case that each feature is on a different scale and thus
have a different influence on the gradient. 
```
import jax.numpy as jnp
from jax import grad
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.datasets import make_classification
from jax import random
from sklearn.metrics import accuracy_score

X, y = make_classification(n_classes=2, n_samples=500)

X_train, X_test, y_train, y_test = train_test_split(X, y)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)

c_0 = 1.0
w_0 = random.normal(random.PRNGKey(0), (X.shape[1], ))
```

the c and w are the learnable parameters. c is the bias and we set that to 1. w is the weight vector and we normally 
distribute this using random values. Note the `random.PRNGKey(0)`, this sets the random number generator with seed 0.
In order to perform gradient descent we need to specify how we go from input to output (our function) and how our loss 
function is determined

```
def predict(c, w, input_):
    return sigmoid(jnp.dot(input_, w) + c)


def cost(c, w, X, y, eps=1e-14, lmbd=0.1):
    p = predict(c, w, X)
    
    return -jnp.mean(y * jnp.log(p) + (1 - y) * jnp.log(1 - p))
```  

Note that the prediction is just a linear transformation given the feature weights and a bias which is then squeezed
between 0 and 1 using the sigmoid function. The cost function is based on the negative log likelihood of the Bernoulli 
distribution. The Bernoulli distribution is given by

$$
p^y * (1 - p)^(1 - y)
$$

The likelihood of this distributuon is given my the product of all the examples given the assumption that they are i.i.d

$$
\prod_n p^y_n * (1 - p)^(1 - y_n)
$$

For numerical convencience we take the log of this likelihood and the negative value

$$
- \frac{1}{n} \sum_n y_n * log(p) + log(1 - p) * (1 - y_n)
$$

Which is equal to the equation given in the cost funtion. Now we want to optimise for c and w given using gradient descent

``` 
n_iter = 500
w = w_0
c = c_0
accuracies_train = []
accuracies_test = []

for i in range(n_iter):
    c_current = c
    c -= eta * grad(cost, argnums=0)(c_current, w, X_train_s, y_train)
    w -= eta * grad(cost, argnums=1)(c_current, w, X_train_s, y_train)
    
    predictions_train = (predict(c, w, X_train_s) > 0.50).astype(int)
    predictions_test = (predict(c, w, X_test_s) > 0.50).astype(int)
    
    accuracies_train.append(accuracy_score(y_pred=predictions_train, y_true=y_train))
    accuracies_test.append(accuracy_score(y_pred=predictions_test, y_true=y_test))
```
    
The most important happens at c and w in the for loop. Here you can see that we iteratively update the values of c 
and w by picking the first or the second argument of the gradient respectively (argnums=x) Classifications are obtained
by thresholding the pobabilities at 0.50. Given the following graph you can see that our model learned the classfiication
function

<img src="http://woutermostard.github.io/files/accs.png" align="middle" width="500" height="300">

Which is further shown by plotting the prediction scores

```
predictions = (predict(c, w, X_test_s) > 0.50).astype(int)

print(classification_report(y_pred=predictions, y_true=y_test))     

              precision    recall  f1-score   support

           0       0.90      0.92      0.91        59
           1       0.92      0.91      0.92        66

    accuracy                           0.91       125
   macro avg       0.91      0.91      0.91       125
weighted avg       0.91      0.91      0.91       125

```

