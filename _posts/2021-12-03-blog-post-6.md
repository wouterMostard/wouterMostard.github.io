---
title: 'Tree based classifiers'
date: 2021-07-04
---

Introduction
======
In this blog post we will be working out the history of tree based methods and how we have come to state-of-the-art classification using XGBoost.
We will start with decision trees, which is the building block of everything to come. Then we go to bagging, which is about selecting a subset of your 
data and performing classification through a committee. Using the subset data collection we arrive at the random forest classifier. The next big thing 
we will touch is boosting, where we learn how to assign weights to individual datapoints that make sense for the next iteration. Then we go to a extension
to that that optimises through a gradient based method. With all that under our belt we are ready to go th XGboost. 

Decision trees
======


Bagging
======


Random forest
======


Boosting
======
The main idea in boosting is that we want to make a `weak` classifier, i.e. which is only marginally better than guessing`, and sequentially learn about what examples
the model should learn about by iteratievely selecting a higher weight to misclassified examples. Let's get started by creating a toy classification problem

```
X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=10, random_state=0, shuffle=False)

X_train, X_test, y_train, y_test = train_test_split(X, y)
y_train = 1 - 2 * y_train
y_test = 1 - 2 * y_test
```
Note that the labels are now not $\{0,1\}$ but $\{-1, 1\}$. This will come in handy when calculating the final score from the committee. The goal of the Boosting model is to create $m$ classifiers $G_m()$ with an importance of $\alpha_i$ which all together make up the classification

$$
    G(X) = sign(\sum^M_{m=1}\alpha_m G_m(x))
$$

Where the $sign$ methods gives 1 when the $x > 0$ and 0 otherwise. So how do we calculate the $N$ classifier importances $\alpha$ and each classifier $G_m(x)$? For the simple classifier we can use something called a `decision stump` which is just a 1 layer decision tree. The simple stump is trained using a sample of the weighted training examples 

```
weighted_indices = np.random.choice(train_indices, size=batch_size, p=w.flatten() / w.sum())
clf = DecisionTreeClassifier(max_depth=1).fit(X_train[weighted_indices], y_train[weighted_indices])
```

How do we determine $w$? Initally we do not know anything about the importance of individual samples so we just set a uniform distribution 

```
w = np.ones(X_train.shape[0]).reshape(-1, 1)
w /= w.sum()
```

This means that we will just uniformly sample a batch for training on the first iteration. After trainign this classifier on a batch we are interested in how well this stump is able to classify the entire training dataset. 

```
predictions = clf.predict(X_train)
wrong = (y_train != predictions).astype(int).reshape(-1, 1)
```

Since we now know the weights of each example and whether the example was classified correctly we can assign a weight to each classified example. 

```
err = (w * wrong).sum() / w.sum() + 1e-6
a = np.log((1 - err) / err)
```

Lastly we update the weights of the training examples

```
w = w * np.exp(a * wrong)
 ```

This process is repeated for a number of iterations until convergence. The entire pipeline is given below 

```
def ada_boost(X_train, y_train, num_iterations=400, batch_size=32):
    train_indices = np.arange(X_train.shape[0])
    classifiers = []
    clf_weights = []
    iteration_scores = []


    # init
    w = np.ones(X_train.shape[0]).reshape(-1, 1)
    w /= w.sum()

    for i in range(num_iterations):
        # Weight the training examples
        weighted_indices = np.random.choice(train_indices, size=batch_size, p=w.flatten() / w.sum())
        # train a classifier and get predictions
        clf = DecisionTreeClassifier(max_depth=1).fit(X_train[weighted_indices], y_train[weighted_indices])
        classifiers.append(clf)
        predictions = clf.predict(X_train)

        # Count the number of wrongly classified 
        wrong = (y_train != predictions).astype(int).reshape(-1, 1)

        # calculated a weighted error
        err = (w * wrong).sum() / w.sum() + 1e-6

        # Calculate how important the classifier is
        a = np.log((1 - err) / err)
        clf_weights.append(a)

        # Set a weight

        w = w * np.exp(a * wrong)
        
        predictions = get_test_predictions(classifiers, clf_weights, X_train)
        iteration_scores.append(1 - accuracy_score(predictions, y_train))
        
    return classifiers, clf_weights, iteration_scores

def get_test_predictions(clfs, clf_weights, X_test):
    scores = []

    for weight, classifier in zip(clf_weights, clfs):
        scores.append(classifier.predict(X_test) * weight)
        
        
    return 1 - 2 * (np.vstack(scores).sum(axis=0) < 0).astype(int)
```

As you can see the ada boost function returns a list of classifiers, a list of classifier weights and a score for each iteration. Using the score for each iteration we can see that with each iteration the Boosting performance is increasing 

<img src="http://woutermostard.github.io/files/boosting.png" align="middle" width="500" height="300">

This method works very well in practice and shows tha ta large number of weak classifiers can have strong performance. 


Gradient Boosting
======

XGBoost
======